{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36a7ccb",
   "metadata": {},
   "source": [
    "### üì¶ 1. Importing Required Libraries\n",
    "We begin by importing the necessary Python libraries for data processing, text cleaning, and building an inverted index:\n",
    "- `re`: for regex-based text cleaning  \n",
    "- `PorterStemmer` from NLTK: for stemming words to their root forms  \n",
    "- `defaultdict`: a handy dictionary subclass from `collections`  \n",
    "- `pandas`: for loading and handling the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f472827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "from nltk.stem.porter import PorterStemmer  \n",
    "from collections import defaultdict \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52649eb9",
   "metadata": {},
   "source": [
    "### üìÑ 2. Loading the Dataset\n",
    "We load a CSV file named `sample_job_dataset.csv` containing job descriptions and associated metadata.  \n",
    "Then we display the first few rows to get a quick overview of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38843dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample_job_dataset.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b4930b",
   "metadata": {},
   "source": [
    "### üßπ 3. Text Preprocessing and Stemming\n",
    "We define a `preprocess` function that:\n",
    "- Converts text to lowercase  \n",
    "- Removes non-alphabetic characters  \n",
    "- Tokenizes the text into words  \n",
    "- Applies stemming using `PorterStemmer`\n",
    "\n",
    "This ensures all job descriptions are normalized before analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())  \n",
    "    words = text.split()  \n",
    "    stemmed = [stemmer.stem(word) for word in words] \n",
    "    return stemmed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d15b46",
   "metadata": {},
   "source": [
    "### üîç 4. Building the Inverted Index\n",
    "We use a `defaultdict` to create an inverted index that maps each stemmed word to the set of job IDs where it appears.  \n",
    "This structure allows fast lookup of job descriptions based on keyword matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e201d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = defaultdict(set)\n",
    "\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    words = preprocess(row[\"description\"])  \n",
    "    for word in words:\n",
    "        inverted_index[word].add(row[\"id\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814b22a",
   "metadata": {},
   "source": [
    "### üîé 5. Search Function Using the Inverted Index\n",
    "The `search` function takes a user query, preprocesses it, and calculates how many query words appear in each job description.  \n",
    "It returns job IDs ranked by how relevant they are to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb06190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    query_words = preprocess(query)  \n",
    "    job_scores = defaultdict(int)  \n",
    "\n",
    "    for word in query_words:\n",
    "        for job_id in inverted_index.get(word, []):  \n",
    "            job_scores[job_id] += 1 \n",
    "\n",
    "   \n",
    "    sorted_jobs = sorted(job_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if not sorted_jobs:\n",
    "        print(\"No matching jobs found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Top matching jobs:\\n\")\n",
    "    for job_id, score in sorted_jobs[:3]:\n",
    "        job = df[df[\"id\"] == job_id].iloc[0]  \n",
    "        print(f\"üîπ {job['title']} (ID: {job_id}) ‚Äî Match Score: {score}\")\n",
    "        print(f\"üìù Description: {job['description']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6073a",
   "metadata": {},
   "source": [
    "### üë§ 6. User Search Query Input\n",
    "The user is prompted to enter job keywords (e.g., \"SQL Developer\").  \n",
    "The search function then retrieves and ranks job postings based on the provided keywords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the user for a search query\n",
    "user_query = input(\"Enter job keywords (e.g., 'SQL Developer'): \")\n",
    "search(user_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88b065",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è 7. Importing ML Libraries for Text Classification\n",
    "We now move to building a classification model by importing:\n",
    "- `TfidfVectorizer`: to convert text to numerical features  \n",
    "- `LinearSVC`: for the classification algorithm  \n",
    "- `train_test_split`, `accuracy_score`: for model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041233d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271eb74",
   "metadata": {},
   "source": [
    "### üìÇ 8. Reloading the Dataset for Classification\n",
    "We reload the job dataset from the CSV file to begin the classification task, where each job description will be categorized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed68647",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample_job_dataset.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855da17",
   "metadata": {},
   "source": [
    "### ‚ú® 9. TF-IDF Vectorization\n",
    "We use `TfidfVectorizer` to convert job descriptions into TF-IDF feature vectors.  \n",
    "These vectors represent the importance of each word and are used as input to the classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4bf803",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"description\"])\n",
    "y = df[\"category\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce41406",
   "metadata": {},
   "source": [
    "### üß™ 10. Splitting the Data\n",
    "We split the TF-IDF features and labels into training and testing sets (80-20 split) using `train_test_split`.  \n",
    "This helps evaluate our model's performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80acbaeb",
   "metadata": {},
   "source": [
    "### üß† 11. Training the Classification Model\n",
    "We use the `LinearSVC` classifier to train our model on the TF-IDF vectors of job descriptions and their corresponding categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df59ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da8568",
   "metadata": {},
   "source": [
    "### üìä 12. Evaluating Model Accuracy\n",
    "After training, we predict job categories for the test set and calculate the model's accuracy using `accuracy_score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ce518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b99b43",
   "metadata": {},
   "source": [
    "### üîÆ 13. Predicting Category from User Query\n",
    "The user provides a job-related search query.  \n",
    "We transform it using the trained TF-IDF vectorizer and use the model to predict the most suitable job category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e633a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = input(\"Enter your job-related query: \")\n",
    "query_vec = vectorizer.transform([user_query])\n",
    "predicted_category = model.predict(query_vec)[0]\n",
    "print(f\"\\nPredicted Category: {predicted_category}\")\n",
    "print(f\"\\nTop jobs in '{predicted_category}' category:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9027580c",
   "metadata": {},
   "source": [
    "### üìã 14. Displaying Jobs in the Predicted Category\n",
    "Once the category is predicted, we filter and display all jobs in that category, including their titles and descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f264f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matches = df[df[\"category\"] == predicted_category]\n",
    "for i, row in matches.iterrows():\n",
    "    print(f\"üîπ {row['title']} (ID: {row['id']})\")\n",
    "    print(f\"üìù Description: {row['description']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf87b25",
   "metadata": {},
   "source": [
    "### üßº 15. Preprocessing All Job Descriptions\n",
    "We apply our earlier `preprocess` function to all job descriptions in the dataset.  \n",
    "This prepares the data for similarity-based search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51521675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_descriptions = [\" \".join(preprocess(description)) for description in df['description']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917076fd",
   "metadata": {},
   "source": [
    "### üó£Ô∏è 16. Preprocessing User Search Query\n",
    "The user provides a natural language search query, which is preprocessed using the same logic as job descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701dac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_query = input(\"Enter job keywords (e.g., 'SQL Developer'): \")\n",
    "processed_query = \" \".join(preprocess(user_query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17248dc0",
   "metadata": {},
   "source": [
    "### üîÑ 17. Preparing the Corpus\n",
    "We combine the preprocessed job descriptions with the preprocessed user query into one list, forming a single corpus for vectorization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = processed_descriptions + [processed_query]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09393a53",
   "metadata": {},
   "source": [
    "### üßÆ 18. Count Vectorization\n",
    "We convert the corpus (job descriptions + query) into count vectors using `CountVectorizer`.  \n",
    "This helps us later compute similarity scores based on word frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e546de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1a068",
   "metadata": {},
   "source": [
    "### üìê 19. Calculating Cosine Similarity\n",
    "We compute the cosine similarity between the user query vector and all job description vectors.  \n",
    "This helps us rank jobs based on how similar they are to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e49f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_scores = cosine_similarity(vectors[-1], vectors[:-1]).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eacc56",
   "metadata": {},
   "source": [
    "### üîù 20. Extracting Top Matches\n",
    "We sort the cosine similarity scores in descending order and select the top 3 indices.  \n",
    "These indices correspond to the most relevant job postings for the user query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43adf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indices = similarity_scores.argsort()[::-1][:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d4660",
   "metadata": {},
   "source": [
    "### üßæ 21. Displaying Top Matching Jobs\n",
    "For each of the top 3 job postings:\n",
    "- We show the job title and ID  \n",
    "- We display the similarity score  \n",
    "- We print the job description\n",
    "\n",
    "This provides the user with the most contextually relevant job results based on their input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop matching jobs:\\n\")\n",
    "for idx in top_indices:\n",
    "    job = df.iloc[idx]  \n",
    "    score = similarity_scores[idx]\n",
    "    print(f\"üîπ {job['title']} (ID: {job['id']}) ‚Äî Similarity Score: {round(score, 2)}\")\n",
    "    print(f\"üìù Description: {job['description']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
